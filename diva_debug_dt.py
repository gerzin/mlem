#!/usr/bin/env python3
import os
import sys
from pathlib import Path
from typing import Any, List

import pandas as pd
from numpy.core.shape_base import vstack
from numpy.lib.arraysetops import unique
from pandas.core.frame import DataFrame

from mlem.attack_models import AttackStrategy
from mlem.explainer import LoreDTLoader
from mlem.utilities import create_attack_dataset, load_pickle_bz2, create_adaboost

# Adds LIME to the system
sys.path.append("./lime")

from pandas import read_pickle
import torch
from mlem.enumerators import BlackBoxType, ExplainerType, SamplingTechnique

from numpy import concatenate, ndarray, load
from typer import echo, run
from lime.lime_tabular import LimeTabularExplainer

from mlem.black_box import BlackBox, PyTorchBlackBox, SklearnBlackBox
from mlem.neural_black_box import Net

from joblib import Parallel, delayed, cpu_count

from mlem.attack_pipeline import perform_attack_pipeline
import pdb

import logging

logger = logging.getLogger()
logger.setLevel(logging.INFO)


def __full_attack_dataset(
        black_box: BlackBox,
        x_train: ndarray,
        y_train: ndarray,
        x_test: ndarray,
        y_test: ndarray,
) -> DataFrame:
    """Creates an attack dataset once and for all, based on the full training set.

    Args:
        black_box (BlackBox): Black box used to perform the prediction.
        x_train (ndarray): Input training set.
        y_train (ndarray): Training target label.
        x_test (ndarray): Input training set.
        y_test (ndarray): Input test set.

    Returns:
        DataFrame: Attack dataset for the full input dataset in the black box.
    """
    # Prediction probabilities for the training set
    y_prob_train: ndarray = black_box.predict_proba(x_train)
    # Prediction probabilities for the test set
    y_prob_test: ndarray = black_box.predict_proba(x_test)
    # Attack dataframe created accordingly
    attack: DataFrame = create_attack_dataset(
        y_prob_train, y_train, y_prob_test, y_test
    )
    return attack


def main(
        black_box_type: BlackBoxType,
        black_box_path: str,
        data_path: str,
        results_path: str = "./results",
        explainer_type: ExplainerType = "lime",
        explainer_sampling: SamplingTechnique = "gaussian",
        neighborhood_sampling: SamplingTechnique = "same",
        num_samples: int = 5000,
        num_shadow_models: int = 4,
        test_size: float = 0.2,
        random_state: int = 42,
        n_jobs: int = -1,
        n_rows: int = None,
        local_attack_dataset_path: str = None,
        lore_dts_path: str = None,
        statistical_generation: bool = False
):
    """Starts a new experimental suite of MLEM.

    Args:\n
        black_box_type (BlackBoxType): Kind of black box to use.\n
        black_box_path (str): Path of the Pickle file where to pick the black box classifier.\n
        data_path (str): Path of the CSV input data.\n # TODO CSV??? or npz with (X_train, X_test, y_train, y_test)
        results_path (str, optional): Path where to save the intermediate results. Defaults to "./results".\n
        explainer_type (ExplainerType, optional): Local explainer to use. Defaults to "lime".\n
        explainer_sampling (SamplingTechnique, optional): Type of sampling performed by the local Explainer to explain a local result. Defaults to "gaussian".\n
        neighborhood_sampling (SamplingTechnique, optional): Type of sampling performed by the local Explainer to perform the MIA. Defaults to "same" (same dataset as before).\n
        num_samples (int, optional): Number of samples for the neighborhood generation. Defaults to 5000.\n
        num_shadow_models (int, optional): Number of shadow models to use in order to mimic the black box. Defaults to 4.\n
        test_size (float, optional): Size of test (in proportion) to extract the data. Defaults to 0.2.\n
        random_state (int, optional): Seed of random number generators. Defaults to 42.\n
        n_jobs (int, optional): Number of jobs used by JobLib to parallelize the works. Defaults to -1 (all the available cores).\n
        n_rows (int, optional): Number of rows of the dataset on which to perform the MIA. Defaults to None (all the rows).\n
        local_attack_dataset_path (str, optional): Path of the data to label with the local model and on which to train the shadow models. (default None: use the data generated by lime)
        lore_dts_path (str, optional): Path of the folder containing the decision trees generated by LORE (each tree must be named dt0.bz2...dtN.bz2)
        statistical_generation (bool, optional): If true generate a statistical attack dataset starting from the lime's one.
    """
    echo("MLEM: MIA (Membership Inference Attack) of Local Explanation Methods")

    if Path(results_path).exists():
        print(f"The results_path {results_path} already exists. Continue and overwrite? [y/n]")
        resp = input("> ")
        if not resp.lower() in "yes":
            exit(1)

    # Load the black box model
    black_box: BlackBox = None
    if black_box_type == BlackBoxType.NN:
        net = Net()
        net.load_state_dict(torch.load(black_box_path))
        black_box = PyTorchBlackBox(net)
    elif black_box_type == BlackBoxType.RF:
        model = load_pickle_bz2(black_box_path)
        black_box = SklearnBlackBox(model)
    else:
        echo("Not a valid black box", err=True)
        exit(1)
    echo("Black box model correctly read")
    # Set the sampling method.
    if explainer_sampling == SamplingTechnique.SAME:
        echo("Sampling of the explainer has to be either 'gaussian' or 'lhs'", err=True)
        exit(1)

    local_attack_dataset = None
    if local_attack_dataset_path:
        print(f"Using {local_attack_dataset_path}")
        local_attack_dataset = pd.read_csv(local_attack_dataset_path, index_col=0)
        local_attack_dataset = local_attack_dataset.drop('y', axis=1).to_numpy()

    echo("Input data correctly read from disk")

    # Creates the result folder if it does not exist
    os.makedirs(results_path, exist_ok=True)
    echo("Output folder correctly created")

    # Local explainer
    explainer = None

    # Specific path for this experimental setting
    path: str = (
        f"{results_path}/debug"
    )
    # Creates the path if it does not exist
    os.makedirs(path, exist_ok=True)
    echo(f"Results are going to be saved in {path}")

    # Indices of the rows of the train dataset
    # Batch size
    batch_size: int = 1

    echo(f"Starting Parallel with {n_jobs=} and {batch_size=} using Adaboost")
    labels = [0, 1]
    attack_full = None
    with Parallel(n_jobs=n_jobs, prefer="processes", batch_size=batch_size) as parallel:
        # For each row of the matrix perform the MIA
        parallel(
            delayed(perform_attack_pipeline)(
                idx,
                x_row,
                y_row,
                labels,
                black_box,
                path,
                explainer,
                explainer_sampling,
                neighborhood_sampling,
                attack_full,
                num_samples,
                num_shadow_models,
                test_size,
                random_state,
                local_attack_dataset,
                create_adaboost,
                AttackStrategy.ONE_PER_LABEL,
                statistical_generation=statistical_generation

            )
            for idx, x_row, y_row in [(0, local_attack_dataset[:1], 0)]
        )
    echo("Experiments are concluded, kudos from MLEM!")


if __name__ == "__main__":
    run(main)
