{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack on Adult using a Decision Tree a Statistical Generated Dataset with filtering of the points, and the points selected by distance\n",
    "\n",
    "In this notebook I use a statistical generated dataset to create the local decision tree. To generate the dataset for the local decision trees I filter out the elements that are distant from the instance to explain more than avg + 3std. The instances used are the one selected by iteratively maximizing the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "from numba import njit\n",
    "\n",
    "UTILS_RELATIVE_PATH = \"../../../../\"\n",
    "sys.path.append(UTILS_RELATIVE_PATH)\n",
    "\n",
    "MLEM_RELATIVE_PATH = \"../../../../../../mlem/\"\n",
    "sys.path.append(MLEM_RELATIVE_PATH)\n",
    "\n",
    "LIME_RELATIVE_PATH = \"../../../../../../mlem/lime/\"\n",
    "sys.path.append(LIME_RELATIVE_PATH)\n",
    "\n",
    "OUTPUT_FOLDER = Path(\"experiment_output\")\n",
    "OUTPUT_FOLDER.mkdir(exist_ok=True)\n",
    "\n",
    "import logging\n",
    "logging.disable('DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_theme()\n",
    "import numpy as np\n",
    "import scipy.spatial.distance as distance\n",
    "import multiprocessing\n",
    "\n",
    "np.random.seed(4321)\n",
    "from sklearn.metrics import classification_report, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from lime.lime_tabular import LimeTabularExplainer # type: ignore\n",
    "from mlem.utilities import generate_balanced_dataset, save_pickle_bz2, load_pickle_bz2, save_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the experiment utilities and the mlem module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Adult data\n",
    "\n",
    "loading the Adult RandomForest and the dictionary with all the useful data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloading.adult import load_adult_data, load_adult_randomforest # type: ignore\n",
    "\n",
    "BB = load_adult_randomforest()\n",
    "BB_DATA = load_adult_data()\n",
    "\n",
    "print(classification_report(BB_DATA['y_test'], BB.predict(BB_DATA['X_test'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the statistical dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a `GaussianMixture` to generate a statistical dataset with the statistics of the black box's training set. The statistical dataset is filtered in order to keep the point closest to the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.filtering import filter_elements_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlem.utilities import stat_sample_dataset, create_gaussian_mixture \n",
    "\n",
    "if not (OUTPUT_FOLDER / \"gaussian_mixture.bz2\").exists():\n",
    "    gm = create_gaussian_mixture(BB_DATA['X_train'])\n",
    "    save_pickle_bz2(OUTPUT_FOLDER / \"gaussian_mixture.bz2\", gm)\n",
    "\n",
    "gm = load_pickle_bz2(OUTPUT_FOLDER / \"gaussian_mixture.bz2\")\n",
    "print(f\"The number of components of the GaussianMixture is {gm.n_components}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_statistical_std(x, num_samples):\n",
    "    \"\"\"Generate statistical data.\n",
    "\n",
    "    Args:\n",
    "        x (instance): ignored since we are not generating aroung it, just needed for the interface\n",
    "        num_samples (int): number of samples to generate\n",
    "    \"\"\"\n",
    "    return stat_sample_dataset(n_samples=num_samples, mixture_model=gm) # using the above GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_representatives_x = BB_DATA['X_distance_separated']\n",
    "test_representatives_y = BB_DATA['y_distance_separated']\n",
    "n_datasets = len(test_representatives_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save(index, instance):\n",
    "    # wrapper of the above functions to be able to generate the datasets in parallel\n",
    "    output_path = OUTPUT_FOLDER / f\"{index}\"\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    # here we filter the elements\n",
    "    balanced_dataset = generate_balanced_dataset(instance, 5000, BB, generate_data_statistical_std, filter_function=filter_elements_std)\n",
    "    balanced_dataset.to_csv(output_path / \"statistical_generated.csv\", index=False)\n",
    "    \n",
    "    with open(output_path / \"instance.npy\", \"wb\") as f:\n",
    "        np.save(f, instance)\n",
    "\n",
    "\n",
    "if not any([Path(OUTPUT_FOLDER / f\"{j}\" / \"statistical_generated.csv\").exists() for j in range(len(test_representatives_x))]):\n",
    "    with multiprocessing.Pool(processes=8) as pool:\n",
    "        pool.starmap(generate_and_save, [*enumerate(test_representatives_x)])\n",
    "else:\n",
    "    print(\"The statistical generated datasets already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the decision trees <span style=\"color:red\"> if they don't already exist </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlem.utilities import create_decision_tree\n",
    "\n",
    "def train_decision_tree(datapath, model_name):\n",
    "    statistical_generated = pd.read_csv(datapath / \"statistical_generated.csv\")\n",
    "    X_train, X_test, y_train, y_test = train_test_split(statistical_generated.drop('Target', axis=1).to_numpy(), statistical_generated.Target.to_numpy(), test_size=0.33, random_state=42)\n",
    "    model_path = datapath / model_name\n",
    "    if not model_path.exists():\n",
    "        dt = create_decision_tree(X_train, y_train, use_halving=True) \n",
    "        save_pickle_bz2(model_path, dt)\n",
    "        save_txt(datapath / \"dt_classification_report.txt\", classification_report(y_test, dt.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_name = \"dt.bz2\"\n",
    "if not any([Path(OUTPUT_FOLDER / f\"{j}\" / \"dt.bz2\").exists() for j in range(len(test_representatives_x))]):\n",
    "    with multiprocessing.Pool(8) as pool:\n",
    "        pool.starmap(train_decision_tree, [(Path(OUTPUT_FOLDER / f\"{i}\"), models_name) for i in range(len(test_representatives_x))])\n",
    "else:\n",
    "    print(\"The DecisionTrees already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack on the Decision Trees to create the attack models\n",
    "\n",
    "Attack on the decision trees using a <span style=\"background: green\">statistical generated dataset</span> labeled by the decision trees to create the shadow models. The dataset elements aren't filtered using the std."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features_mask = BB_DATA['categorical_features_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the shadow models and the attack models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlem.shadow_models import ShadowModelsManager\n",
    "from mlem.utilities import create_adaboost\n",
    "from mlem.attack_models import AttackModelsManager, AttackStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics_local_model(dataset, true_y, local_model, black_box, output_folder, filename):\n",
    "    local_y = local_model.predict(dataset.to_numpy())\n",
    "    local_bb = black_box.predict(dataset.to_numpy())\n",
    "\n",
    "    report_local = classification_report(true_y.to_numpy(), local_y)\n",
    "    report_bb    = classification_report(true_y.to_numpy(), local_bb)\n",
    "\n",
    "    fidelity = str(pd.DataFrame(local_y == local_bb).value_counts(normalize=True))\n",
    "\n",
    "    with open(output_folder / filename, \"w\") as f:\n",
    "        f.write(\"Statistics on the noisy validation dataset\\n\")\n",
    "        \n",
    "        f.write(\"local model\\n\")\n",
    "        f.write(report_local)\n",
    "\n",
    "        f.write(\"\\nblack box\\n\")\n",
    "        f.write(report_bb)\n",
    "\n",
    "        f.write(\"\\nFidelity between the local model and the black box\\n\")\n",
    "        f.write(fidelity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the attack only if it hasn't already been run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not any([(OUTPUT_FOLDER / f\"{i}\" / \"attack\").exists() for i in range(len(test_representatives_x))]):\n",
    "    \n",
    "    for ind, path in enumerate([OUTPUT_FOLDER / f\"{i}\" for i in range(len(test_representatives_x))]):\n",
    "        # load the local tree and label the noisy dataset\n",
    "        local_tree = load_pickle_bz2(path / \"dt.bz2\")\n",
    "\n",
    "        # here the first argument is ignored\n",
    "        statistical = generate_balanced_dataset(test_representatives_x[ind], 5000, BB, generate_data_statistical_std)\n",
    "        statistical_x = statistical.drop('Target', axis=1)\n",
    "        statistical_y = statistical.Target\n",
    "\n",
    "        # compute fidelity and performances on the noisy dataset.\n",
    "        compute_statistics_local_model(statistical_x, statistical_y, local_tree, BB, path, \"statistics_statisticalgenerated.txt\")\n",
    "\n",
    "        # use the \"local\" decision tree to label the statistical dataset\n",
    "        x_attack = statistical_x.to_numpy()    \n",
    "        y_attack = local_tree.predict(x_attack)\n",
    "\n",
    "        path_shadow = str(path / \"shadow\")\n",
    "\n",
    "        shadow_models = ShadowModelsManager(\n",
    "            n_models=4,\n",
    "            results_path=path_shadow,\n",
    "            test_size=0.5,\n",
    "            random_state=123,\n",
    "            model_creator_fn=create_adaboost,\n",
    "            categorical_mask=categorical_features_mask\n",
    "        )\n",
    "        \n",
    "        shadow_models.fit(x_attack, y_attack)\n",
    "\n",
    "        # extracting the dataset for the attack models\n",
    "        attack_models_dataset = shadow_models.get_attack_dataset()\n",
    "\n",
    "        # saving the attack dataset\n",
    "        attack_models_dataset.to_csv(path / \"attack_models_train_dataset.csv\", index=False)\n",
    "\n",
    "        # Creating the attack model for each label using Adaboost\n",
    "        path_attack = str(path / \"attack\")\n",
    "        attack_models = AttackModelsManager(\n",
    "                results_path=path_attack, model_creator_fn=create_adaboost, attack_strategy=AttackStrategy.ONE_PER_LABEL\n",
    "        )\n",
    "        \n",
    "        attack_models.fit(attack_models_dataset)\n",
    "else:\n",
    "    print(\"The attack models already exist\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the test reports of the attack models, it seems that they <span style=\"background: red\">perform bad on both classes</span> with an accuracy of $\\simeq .50$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlem.ensemble import HardVotingClassifier, SoftVotingClassifier, KMostSureVotingClassifier\n",
    "from utils.attack_evaluation import evaluate_attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_models_0 = [load_pickle_bz2(OUTPUT_FOLDER / f\"{i}\" / \"attack\" / \"0\" / \"model.pkl.bz2\") for i in range(len(test_representatives_x))]\n",
    "attack_models_1 = [load_pickle_bz2(OUTPUT_FOLDER / f\"{i}\" / \"attack\" / \"1\" / \"model.pkl.bz2\") for i in range(len(test_representatives_x))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv0 = HardVotingClassifier(classifiers=attack_models_0)\n",
    "hv1 = HardVotingClassifier(classifiers=attack_models_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_attack(hv0, hv1, BB, BB_DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv0 = SoftVotingClassifier(classifiers=attack_models_0)\n",
    "sv1 = SoftVotingClassifier(classifiers=attack_models_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_attack(sv0, sv1, BB, BB_DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "19c5d3a3188652f893cd4e64043d7470541fe23a697d87860bb02bfe4b8542de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
